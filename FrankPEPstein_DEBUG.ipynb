{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6321fa",
   "metadata": {},
   "source": [
    "\n",
    "# FrankPEPstein: Incremental Debug Notebook\n",
    "\n",
    "This notebook tests the pipeline using a consolidated workflow:\n",
    "1.  **Setup**: Installs dependencies in a dedicated environment.\n",
    "2.  **Workflow**: Uploads Receptor -> Detects/Uploads Pockets -> Selects Pocket -> Calculates Box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e8963",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title 0. Install CondaColab & Setup Tools\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Helper to suppress output\n",
    "class SuppressStdout:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "def run_setup():\n",
    "    # Install tqdm first if missing (fast)\n",
    "    try:\n",
    "        from tqdm.notebook import tqdm\n",
    "    except ImportError:\n",
    "        subprocess.run(\"pip install -q tqdm\", shell=True)\n",
    "        from tqdm.notebook import tqdm\n",
    "\n",
    "    print(\"Setting up FrankPEPstein environment...\")\n",
    "    \n",
    "    steps = [\n",
    "        (\"Installing CondaColab\", \"condacolab\"),\n",
    "        (\"Cloning Repository\", \"git\"),\n",
    "        (\"Creating Conda Environment (Slow)\", \"env\"),\n",
    "        (\"Configuring Notebook Utils\", \"patch\"),\n",
    "        (\"Setting up External Tools\", \"tools\"),\n",
    "        (\"Configuring Modeller\", \"modeller\")\n",
    "    ]\n",
    "    \n",
    "    with tqdm(total=len(steps)) as pbar:\n",
    "        # 1. CondaColab\n",
    "        pbar.set_description(steps[0][0])\n",
    "        try:\n",
    "            with SuppressStdout():\n",
    "                import condacolab\n",
    "                condacolab.check()\n",
    "        except ImportError:\n",
    "            with SuppressStdout():\n",
    "                subprocess.run(\"pip install -q condacolab\", shell=True, check=True)\n",
    "                import condacolab\n",
    "                condacolab.install()\n",
    "        pbar.update(1)\n",
    "\n",
    "        # 2. Git Clone\n",
    "        pbar.set_description(steps[1][0])\n",
    "        with SuppressStdout():\n",
    "            if not os.path.exists(\"FrankPEPstein\"):\n",
    "                subprocess.run(\"git clone https://github.com/Joacaldog/FrankPEPstein.git\", shell=True, check=True)\n",
    "        pbar.update(1)\n",
    "\n",
    "        # 3. Create Environment\n",
    "        pbar.set_description(steps[2][0])\n",
    "        env_path = \"/usr/local/envs/FrankPEPstein\"\n",
    "        if not os.path.exists(env_path):\n",
    "             # Redirect mamba output to devnull\n",
    "             subprocess.run(\"mamba create -n FrankPEPstein -q -y -c conda-forge -c salilab openbabel biopython fpocket joblib tqdm py3dmol vina python=3.10 salilab::modeller > /dev/null 2>&1\", shell=True, check=True)\n",
    "        \n",
    "        # Configure Path\n",
    "        site_packages = f\"{env_path}/lib/python3.10/site-packages\"\n",
    "        if site_packages not in sys.path:\n",
    "            sys.path.append(site_packages)\n",
    "        os.environ['PATH'] = f\"{env_path}/bin:\" + os.environ['PATH']\n",
    "        pbar.update(1)\n",
    "\n",
    "        # 4. Patch Utils\n",
    "        pbar.set_description(steps[3][0])\n",
    "        patched_utils_content = r'''import os\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def configure_modeller(license_key='MODELIRANJE', repo_dir='FrankPEPstein'):\n",
    "    \"\"\"\n",
    "    Configures Modeller by locating the config.py file in the installation\n",
    "    and replacing the license key placeholder with the provided key.\n",
    "    \"\"\"\n",
    "    # Template location in the repo\n",
    "    template_config = os.path.join(repo_dir, \"utilities/config.py\")\n",
    "    \n",
    "    # Try using python import to find the location\n",
    "    dest_config = None\n",
    "    try:\n",
    "        import modeller\n",
    "        modeller_path = os.path.dirname(modeller.__file__)\n",
    "        candidate = os.path.join(modeller_path, \"config.py\")\n",
    "        if os.path.exists(candidate):\n",
    "            dest_config = candidate\n",
    "    except Exception:\n",
    "        # Modeller raises an error on import if not configured, which is expected.\n",
    "        # We just want to find where it is installed.\n",
    "        pass\n",
    "\n",
    "    # Fallback to search if import finding failed\n",
    "    if not dest_config:\n",
    "        possible_paths = [\n",
    "            f\"{sys.prefix}/lib/modeller-*/modlib/modeller/config.py\", # Standard standalone install\n",
    "            f\"{sys.prefix}/lib/python*/site-packages/modeller/config.py\", # Site-packages install\n",
    "            f\"{sys.prefix}/pkgs/modeller-*/lib/modeller-*/modlib/modeller/config.py\", # Conda pkgs cache structure\n",
    "            \"/usr/local/envs/FrankPEPstein/lib/modeller-*/modlib/modeller/config.py\" # Targeted Conda Environment\n",
    "        ]\n",
    "        \n",
    "        dest_config_paths = []\n",
    "        for pattern in possible_paths:\n",
    "            found = glob.glob(pattern)\n",
    "            dest_config_paths.extend(found)\n",
    "        \n",
    "        if dest_config_paths:\n",
    "            dest_config = dest_config_paths[0]\n",
    "\n",
    "    \n",
    "    if dest_config and os.path.exists(template_config):\n",
    "        print(f\"Found modeller config at: {dest_config}\")\n",
    "        print(f\"Using template {template_config} to update {dest_config}\")\n",
    "        \n",
    "        with open(template_config, 'r') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Replace placeholder 'MODELIRANJE' with actual key\n",
    "        new_content = content.replace(\"'MODELIRANJE'\", f\"'{license_key}'\")\n",
    "        \n",
    "        with open(dest_config, 'w') as f:\n",
    "            f.write(new_content)\n",
    "        print(\"Modeller configured successfully.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Error: Modeller config destination ({dest_config}) or template ({template_config}) not found.\")\n",
    "        return False\n",
    "\n",
    "def get_pocket_box(pdb_file):\n",
    "    \"\"\"\n",
    "    Calculates the center and size of a box surrounding the atoms in the given PDB file.\n",
    "    Adds a buffer of 5.0 units to the size.\n",
    "    \"\"\"\n",
    "    import Bio.PDB\n",
    "    parser = Bio.PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"pocket\", pdb_file)\n",
    "    coords = []\n",
    "    for atom in structure.get_atoms():\n",
    "        coords.append(atom.get_coord())\n",
    "    \n",
    "    if not coords:\n",
    "        return None, None\n",
    "\n",
    "    min_coord = [min([c[i] for c in coords]) for i in range(3)]\n",
    "    max_coord = [max([c[i] for c in coords]) for i in range(3)]\n",
    "    \n",
    "    center = [(min_coord[i] + max_coord[i]) / 2 for i in range(3)]\n",
    "    size = [(max_coord[i] - min_coord[i]) + 5.0 for i in range(3)] # Add buffer\n",
    "    return center, size\n",
    "\n",
    "def patch_scripts(scripts_dir, path_replacements):\n",
    "    \"\"\"\n",
    "    Iterates through .py files in scripts_dir and applies string replacements.\n",
    "    \"\"\"\n",
    "    print(\"Patching scripts...\")\n",
    "    count = 0\n",
    "    for script_name in os.listdir(scripts_dir):\n",
    "        if script_name.endswith(\".py\"):\n",
    "            full_path = os.path.join(scripts_dir, script_name)\n",
    "            with open(full_path, 'r') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            original_content = content\n",
    "            for old, new in path_replacements.items():\n",
    "                content = content.replace(old, new)\n",
    "            \n",
    "            # Additional patches for command calls\n",
    "            content = content.replace(\"vina\", \"vina\") \n",
    "            \n",
    "            if content != original_content:\n",
    "                with open(full_path, 'w') as f:\n",
    "                    f.write(content)\n",
    "                print(f\"Patched {script_name}\")\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def setup_external_tools(drive_ids=None):\n",
    "    \"\"\"\n",
    "    Sets up external tools (ADFR, Click, DB).\n",
    "    If drive_ids is provided, downloads missing files from Google Drive.\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    \n",
    "    # Ensure gdown is installed\n",
    "    try:\n",
    "        import gdown\n",
    "    except ImportError:\n",
    "        print(\"Installing gdown...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"gdown\"], check=True)\n",
    "        import gdown\n",
    "\n",
    "    base_dir = \"FrankPEPstein\"\n",
    "    # Adjust base_dir if we are running from root vs inside scripts?\n",
    "    # The utils assume repo_dir='FrankPEPstein' usually implies subfolder.\n",
    "    # But if looking for \"utilities\", it usually expects to find them relative to CWD?\n",
    "    # Let's check config.\n",
    "    # In notebook setup: repo_path = os.path.abspath(\"FrankPEPstein\").\n",
    "    # If we run cell_01_setup.py from FrankPEPstein root, base_dir \"FrankPEPstein\" might be wrong if we are IN it?\n",
    "    # notebook_utils.py:\n",
    "    #   configure_modeller default repo_dir='FrankPEPstein'.\n",
    "    #   BUT when running locally in the repo, 'FrankPEPstein' folder DOES NOT EXIST inside 'FrankPEPstein'.\n",
    "    #   The repo IS the cwd.\n",
    "    #   When cloning in Colab: cwd is /content/, repo is /content/FrankPEPstein.\n",
    "    #   So 'FrankPEPstein/utilities' is correct there.\n",
    "    #   But LOCALLY, if I am in ~/FrankPEPstein/, 'FrankPEPstein/utilities' does not exist. 'utilities' exists.\n",
    "    \n",
    "    # I need to handle this path difference!\n",
    "    \n",
    "    if os.path.exists(\"utilities\"):\n",
    "        # We are likely INSIDE the repo root (Local execution)\n",
    "        base_dir = \".\"\n",
    "    elif os.path.exists(\"FrankPEPstein/utilities\"):\n",
    "        # We are likely in parent dir (Colab default)\n",
    "        base_dir = \"FrankPEPstein\"\n",
    "    else:\n",
    "        # Fallback or create?\n",
    "        base_dir = \"FrankPEPstein\" # Default to colab behavior for safety, or create it.\n",
    "\n",
    "    utilities_dir = os.path.join(base_dir, \"utilities\")\n",
    "    db_dir = os.path.join(base_dir, \"DB\")\n",
    "    \n",
    "    os.makedirs(utilities_dir, exist_ok=True)\n",
    "    os.makedirs(db_dir, exist_ok=True)\n",
    "\n",
    "    # File definitions\n",
    "    # --- Bundle Download Logic ---\n",
    "    # Downloads everything in two main packages if IDs are provided\n",
    "    bundles = {\n",
    "        \"utilities_pkg\": {\n",
    "            \"path\": os.path.join(base_dir, \"utilities.tar.gz\"),\n",
    "            \"id_key\": \"utilities_pkg_id\",\n",
    "            \"extract_to\": os.path.join(base_dir, \"utilities\"),\n",
    "            \"desc\": \"Utilities Bundle\"\n",
    "        },\n",
    "        \"db_pkg\": {\n",
    "            \"path\": os.path.join(base_dir, \"DB.tar.gz\"),\n",
    "            \"id_key\": \"db_pkg_id\",\n",
    "            \"extract_to\": os.path.join(base_dir, \"DB\"),\n",
    "            \"desc\": \"Database Bundle\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if drive_ids is None:\n",
    "        drive_ids = {}\n",
    "\n",
    "    # Download and extract bundles first\n",
    "    for name, info in bundles.items():\n",
    "        bundle_id = drive_ids.get(info[\"id_key\"])\n",
    "        if bundle_id and not os.path.exists(info[\"extract_to\"]): # Only if dir doesn't exist? Or check manifest?\n",
    "             # Actually, we should check if the CONTENT exists, but downloading bundle is safer if unsure.\n",
    "             # Simple check: if tarball doesn't exist, download.\n",
    "             if not os.path.exists(info[\"path\"]):\n",
    "                 print(f\"Downloading {info['desc']}...\")\n",
    "                 url = f'https://drive.google.com/uc?id={bundle_id}'\n",
    "                 gdown.download(url, info[\"path\"], quiet=False)\n",
    "             \n",
    "             # Extract\n",
    "             if os.path.exists(info[\"path\"]):\n",
    "                 print(f\"Extracting {info['desc']}...\")\n",
    "                 os.makedirs(info[\"extract_to\"], exist_ok=True)\n",
    "                 # strip-components=0 because we tarred content of utilies into utilities.tar.gz?\n",
    "                 # I tarred with -C utilities, so it contains \"ADFR...\" at root.\n",
    "                 subprocess.run(f\"tar -xzf {info['path']} -C {info['extract_to']}\", shell=True, check=True)\n",
    "\n",
    "    # --- Individual Tool Verification ---\n",
    "    # Even after bundle extraction, we run this to ensure paths are set and bins are executable.\n",
    "    # It also handles legacy cases (individual IDs provided).\n",
    "    files = {\n",
    "        \"adfr\": {\n",
    "            \"path\": os.path.join(utilities_dir, \"ADFRsuite_x86_64Linux_1.0.tar.gz\"),\n",
    "            \"id_key\": \"adfr_id\", # Fallback key\n",
    "            \"extract_cmd\": f\"tar -xzf {{}} -C {utilities_dir}\",\n",
    "            \"bin_path\": os.path.join(os.path.abspath(utilities_dir), \"ADFRsuite_x86_64Linux_1.0/bin\") \n",
    "        },\n",
    "        \"click\": {\n",
    "            \"path\": os.path.join(utilities_dir, \"Click.tar.gz\"),\n",
    "            \"id_key\": \"click_id\",\n",
    "            \"extract_cmd\": f\"tar -xzf {{}} -C {utilities_dir}\",\n",
    "            \"bin_path\": os.path.join(os.path.abspath(utilities_dir), \"Click/bin\")\n",
    "        },\n",
    "        \"db\": {\n",
    "            \"path\": os.path.join(db_dir, \"minipockets_surface80_winsize3_size3_curated-db.tar.gz\"),\n",
    "            \"id_key\": \"db_id\",\n",
    "            \"extract_cmd\": f\"tar -xzf {{}} -C {db_dir}\"\n",
    "        },\n",
    "        \"dict\": {\n",
    "            \"path\": os.path.join(db_dir, \"reduce_wwPDB_het_dict.tar.gz\"), \n",
    "            \"id_key\": \"dict_id\",\n",
    "            \"extract_cmd\": f\"tar -xzf {{}} -C {db_dir}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for name, info in files.items():\n",
    "        if not os.path.exists(info[\"path\"]):\n",
    "            # Check if we have an ID to download (fallback)\n",
    "            file_id = drive_ids.get(info[\"id_key\"])\n",
    "            if file_id:\n",
    "                print(f\"Downloading {name} (Fallback)...\")\n",
    "                url = f'https://drive.google.com/uc?id={file_id}'\n",
    "                gdown.download(url, info[\"path\"], quiet=False)\n",
    "        \n",
    "        # Extract if exists\n",
    "        should_extract = False\n",
    "        if os.path.exists(info[\"path\"]):\n",
    "             should_extract = True\n",
    "             if \"bin_path\" in info and os.path.exists(info[\"bin_path\"]):\n",
    "                 should_extract = False\n",
    "             # For DB files that don't have bin_path, we might re-extract unnecessarily?\n",
    "             # Check if destination exists\n",
    "             if name == \"db\" and os.path.exists(os.path.join(db_dir, \"minipockets_surface80_winsize3_size3_curated-db\")):\n",
    "                 should_extract = False\n",
    "             if name == \"dict\" and os.path.exists(os.path.join(db_dir, \"reduce_wwPDB_het_dict.txt\")):\n",
    "                 should_extract = False\n",
    "        \n",
    "        if should_extract:\n",
    "            print(f\"Extracting {name}...\")\n",
    "            subprocess.run(info[\"extract_cmd\"].format(info[\"path\"]), shell=True, check=True)\n",
    "            \n",
    "        # Add to PATH if needed\n",
    "        if \"bin_path\" in info and os.path.exists(info[\"bin_path\"]):\n",
    "            if info['bin_path'] not in os.environ['PATH']:\n",
    "                os.environ['PATH'] += f\":{info['bin_path']}\"\n",
    "                print(f\"Added {name} to PATH: {info['bin_path']}\")\n",
    "            if name == \"click\":\n",
    "                    subprocess.run(f\"chmod +x {info['bin_path']}/click\", shell=True)\n",
    "    \n",
    "    # Handle dictionary txt\n",
    "    dict_txt = os.path.join(db_dir, \"reduce_wwPDB_het_dict.txt\")\n",
    "    if os.path.exists(dict_txt):\n",
    "        print(\"Dictionary txt found.\")\n",
    "    else:\n",
    "        print(\"WARNING: reduce_wwPDB_het_dict.txt not found (maybe inside another folder after extraction?)\")\n",
    "\n",
    "'''\n",
    "        os.makedirs(\"FrankPEPstein/scripts\", exist_ok=True)\n",
    "        with open(\"FrankPEPstein/scripts/notebook_utils.py\", \"w\") as f:\n",
    "            f.write(patched_utils_content)\n",
    "        pbar.update(1)\n",
    "\n",
    "        # 5. External Tools Setup\n",
    "        pbar.set_description(steps[4][0])\n",
    "        repo_path = os.path.abspath(\"FrankPEPstein\")\n",
    "        if repo_path not in sys.path:\n",
    "            sys.path.append(repo_path)\n",
    "        from scripts import notebook_utils\n",
    "        \n",
    "        # DRIVE CONFIGURATION: Enter your File IDs here\n",
    "        drive_ids = {\n",
    "            \"adfr_id\": \"1gmRj8mva84-JB7UXUcQfB3Ziw_nwwdox\",\n",
    "            \"db_id\": \"1a4GoZ1ZT-DNYMyvVtKJukNdF6TAaLJU5\", \n",
    "            \"dict_id\": \"1nrwSUof0lox9fp8Ow5EICIN9u0lglu7U\"\n",
    "        }\n",
    "        \n",
    "        # Suppress output of these functions too if possible, or live with it?\n",
    "        # User wants \"barrita de carga y nada mas\".\n",
    "        # We can capture stdout.\n",
    "        with SuppressStdout():\n",
    "             notebook_utils.setup_external_tools(drive_ids)\n",
    "        pbar.update(1)\n",
    "\n",
    "        # 6. Configure Modeller\n",
    "        pbar.set_description(steps[5][0])\n",
    "        with SuppressStdout():\n",
    "            notebook_utils.configure_modeller()\n",
    "        pbar.update(1)\n",
    "        \n",
    "    clear_output()\n",
    "    print(\"âœ… Setup Ready!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6155791",
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title 1. Input & Pocket Selection\n",
    "#@markdown **Instructions:**\n",
    "#@markdown 1. Upload your Receptor PDB.\n",
    "#@markdown 2. Choose Mode: **Auto Detect** (runs fpocket) or **Manual Upload** (upload your specific pocket PDB).\n",
    "#@markdown 3. Select the pocket from the dropdown to visualize.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import py3Dmol\n",
    "import ipywidgets as widgets\n",
    "from google.colab import files\n",
    "from IPython.display import display\n",
    "\n",
    "# --- configuration ---\n",
    "detection_mode = \"Auto Detect\" #@param [\"Auto Detect\", \"Manual Upload\"]\n",
    "\n",
    "# Global variables for next steps\n",
    "receptor_filename = None\n",
    "pockets_dir = \"pockets_upload\" # Default for manual\n",
    "final_pockets_list = []\n",
    "\n",
    "# --- 1. Upload Receptor ---\n",
    "print(f\"--- Upload Receptor PDB ({detection_mode}) ---\")\n",
    "uploaded_r = files.upload()\n",
    "\n",
    "if not uploaded_r:\n",
    "    print(\"No receptor file uploaded.\")\n",
    "else:\n",
    "    receptor_filename = list(uploaded_r.keys())[0]\n",
    "    print(f\"Receptor: {receptor_filename}\")\n",
    "\n",
    "    # --- 2. Pocket Handling ---\n",
    "    if detection_mode == \"Auto Detect\":\n",
    "        print(f\"\\nRunning fpocket on {receptor_filename}...\")\n",
    "        try:\n",
    "            # Fix: Quotes for filenames with spaces\n",
    "            subprocess.run(f\"fpocket -f '{receptor_filename}'\", shell=True, check=True)\n",
    "            \n",
    "            # Robust folder finding\n",
    "            base_name = os.path.splitext(receptor_filename)[0]\n",
    "            possible_folders = [f\"{receptor_filename}_out\", f\"{base_name}_out\"]\n",
    "            output_folder = next((f for f in possible_folders if os.path.exists(f)), None)\n",
    "\n",
    "            if output_folder:\n",
    "                pockets_dir = os.path.join(output_folder, \"pockets\")\n",
    "                if os.path.exists(pockets_dir):\n",
    "                    final_pockets_list = [f for f in os.listdir(pockets_dir) if f.endswith(\".pdb\")]\n",
    "                    print(f\"Auto-detection finished. Found {len(final_pockets_list)} pockets.\")\n",
    "                else:\n",
    "                    print(\"Warning: pockets subdirectory not found.\")\n",
    "            else:\n",
    "                print(\"Error: fpocket output not found.\")\n",
    "                \n",
    "        except subprocess.CalledProcessError:\n",
    "             print(\"Error running fpocket.\")\n",
    "\n",
    "    elif detection_mode == \"Manual Upload\":\n",
    "        print(f\"\\n--- Upload Manual Pocket PDB ---\")\n",
    "        os.makedirs(pockets_dir, exist_ok=True)\n",
    "        uploaded_p = files.upload()\n",
    "        if uploaded_p:\n",
    "            for p_file in uploaded_p.keys():\n",
    "                # Move to a pockets folder to keep structure consistent\n",
    "                os.rename(p_file, os.path.join(pockets_dir, p_file))\n",
    "                final_pockets_list.append(p_file)\n",
    "            print(f\"Manual upload finished. Available pockets: {len(final_pockets_list)}\")\n",
    "\n",
    "    # --- 3. Visualization & Selection ---\n",
    "    if final_pockets_list:\n",
    "        print(\"\\n--- Pocket Selection & Visualization ---\")\n",
    "        \n",
    "        pocket_dropdown = widgets.Dropdown(\n",
    "            options=sorted(final_pockets_list),\n",
    "            description='Select Pocket:',\n",
    "            disabled=False,\n",
    "        )\n",
    "\n",
    "        def view_pocket(pocket_file):\n",
    "            view = py3Dmol.view(width=800, height=600)\n",
    "            \n",
    "            # 1. Receptor Surface (White, Transparent)\n",
    "            with open(receptor_filename, 'r') as f:\n",
    "                view.addModel(f.read(), \"pdb\")\n",
    "            view.setStyle({}) \n",
    "            view.addSurface(py3Dmol.SES, {'opacity': 0.8, 'color': 'white'})\n",
    "            \n",
    "            # 2. Selected Pocket (Red Spheres)\n",
    "            # Ensure we look in the correct dir (either fpocket out or manual upload dir)\n",
    "            full_path = os.path.join(pockets_dir, pocket_file)\n",
    "            if os.path.exists(full_path):\n",
    "                with open(full_path, 'r') as f:\n",
    "                    view.addModel(f.read(), \"pdb\")\n",
    "                view.setStyle({'model': -1}, {'sphere': {'color': 'red', 'opacity': 0.7}})\n",
    "            else:\n",
    "                print(f\"Error: Could not find {full_path}\")\n",
    "\n",
    "            view.zoomTo()\n",
    "            view.show()\n",
    "            \n",
    "            # Set a global var for the *path* so next cell can find it easily? \n",
    "            # Actually next cell will read `pocket_dropdown.value` and `pockets_dir`\n",
    "            \n",
    "        display(widgets.interactive(view_pocket, pocket_file=pocket_dropdown))\n",
    "    else:\n",
    "        print(\"No pockets available to select.\")\n",
    "#@title 4. Pocket Extraction & Box Generation\n",
    "#@markdown This step extracts the selected pocket and calculates the grid box center and size.\n",
    "\n",
    "import os\n",
    "from Bio.PDB import PDBParser, PDBIO, Select\n",
    "\n",
    "# --- Configuration ---\n",
    "# Read variables from previous step (Assuming they exist in global scope)\n",
    "# In Colab, variables persist across cells.\n",
    "try:\n",
    "    selected_pocket_file = pocket_dropdown.value\n",
    "    print(f\"Selected Pocket File: {selected_pocket_file}\")\n",
    "    \n",
    "    # pockets_dir was defined in Step 1-3\n",
    "    pocket_path = os.path.join(pockets_dir, selected_pocket_file)\n",
    "    print(f\"Pocket Path: {pocket_path}\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"Error: 'pocket_dropdown' or 'pockets_dir' not defined. Did you run the previous cell?\")\n",
    "    # Fallback for testing/debugging if not running sequentially\n",
    "    # pocket_path = \"pockets/pocket1.pdb\" \n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_box_center_size(pdb_file, buffer=0.0):\n",
    "    \"\"\"\n",
    "    Calculates center and size of the box from PDB atoms.\n",
    "    Buffer adds padding to the box size (total).\n",
    "    \"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"pocket\", pdb_file)\n",
    "    coords = []\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                for atom in residue:\n",
    "                    coords.append(atom.get_coord())\n",
    "    \n",
    "    if not coords:\n",
    "        return None, None\n",
    "\n",
    "    min_coord = [min([c[i] for c in coords]) for i in range(3)]\n",
    "    max_coord = [max([c[i] for c in coords]) for i in range(3)]\n",
    "    \n",
    "    center = [(min_coord[i] + max_coord[i]) / 2 for i in range(3)]\n",
    "    # Size is the dimension length\n",
    "    size = [(max_coord[i] - min_coord[i]) + buffer for i in range(3)]\n",
    "    \n",
    "    return center, size\n",
    "\n",
    "# --- Main Extraction Logic ---\n",
    "if os.path.exists(pocket_path):\n",
    "    print(\"Calculating box parameters...\")\n",
    "    # Buffer can be adjusted. Usually 4-10 Angstroms padding is standard for Vina\n",
    "    center, size = get_box_center_size(pocket_path, buffer=10.0) \n",
    "    \n",
    "    if center:\n",
    "        # Round for cleaner output\n",
    "        center_str = f\"{center[0]:.3f} {center[1]:.3f} {center[2]:.3f}\"\n",
    "        size_str = f\"{size[0]:.3f} {size[1]:.3f} {size[2]:.3f}\"\n",
    "        \n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Box Center: {center_str}\")\n",
    "        print(f\"Box Size:   {size_str}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Save as global variables for next steps (FrankPEPstein)\n",
    "        box_center = center\n",
    "        box_size = size\n",
    "        \n",
    "        # Optional: Save a 'clean' pocket PDB if needed?\n",
    "        # fpocket output is usually already clean atoms of the pocket.\n",
    "        # But we might want to ensure it is standard PDB.\n",
    "        \n",
    "        print(\"Pocket parameters ready for FrankPEPstein.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Error: Could not calculate coordinates from pocket file.\")\n",
    "else:\n",
    "    print(f\"Error: Pocket file not found at {pocket_path}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
